{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Crawler.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR4uzlPsKkb2",
        "colab_type": "code",
        "outputId": "9438540d-ecba-421c-d1ee-fc135eaded04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip3 install scrapy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scrapy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/4b/585e8e111ffb01466c59281f34febb13ad1a95d7fb3919fd57c33fc732a5/Scrapy-1.7.3-py2.py3-none-any.whl (234kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 9.9MB/s \n",
            "\u001b[?25hCollecting parsel>=1.5 (from scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/86/c8/fc5a2f9376066905dfcca334da2a25842aedfda142c0424722e7c497798b/parsel-1.5.2-py2.py3-none-any.whl\n",
            "Collecting PyDispatcher>=2.0.5 (from scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/37/39aca520918ce1935bea9c356bcbb7ed7e52ad4e31bff9b943dfc8e7115b/PyDispatcher-2.0.5.tar.gz\n",
            "Collecting cssselect>=0.9 (from scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from scrapy) (1.12.0)\n",
            "Requirement already satisfied: lxml; python_version != \"3.4\" in /usr/local/lib/python3.6/dist-packages (from scrapy) (4.2.6)\n",
            "Collecting service-identity (from scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/7c/2195b890023e098f9618d43ebc337d83c8b38d414326685339eb024db2f6/service_identity-18.1.0-py2.py3-none-any.whl\n",
            "Collecting pyOpenSSL (from scrapy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/c8/ceb170d81bd3941cbeb9940fc6cc2ef2ca4288d0ca8929ea4db5905d904d/pyOpenSSL-19.0.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 28.6MB/s \n",
            "\u001b[?25hCollecting w3lib>=1.17.0 (from scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/45/1ba17c50a0bb16bd950c9c2b92ec60d40c8ebda9f3371ae4230c437120b6/w3lib-1.21.0-py2.py3-none-any.whl\n",
            "Collecting Twisted>=13.1.0; python_version != \"3.4\" (from scrapy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/49/eb654da38b15285d1f594933eefff36ce03106356197dba28ee8f5721a79/Twisted-19.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 51.5MB/s \n",
            "\u001b[?25hCollecting queuelib (from scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/4c/85/ae64e9145f39dd6d14f8af3fa809a270ef3729f3b90b3c0cf5aa242ab0d4/queuelib-1.5.0-py2.py3-none-any.whl\n",
            "Collecting cryptography (from service-identity->scrapy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/18/c6557f63a6abde34707196fb2cad1c6dc0dbff25a200d5044922496668a4/cryptography-2.7-cp34-abi3-manylinux1_x86_64.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 49.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=16.0.0 in /usr/local/lib/python3.6/dist-packages (from service-identity->scrapy) (19.1.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.6/dist-packages (from service-identity->scrapy) (0.4.7)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.6/dist-packages (from service-identity->scrapy) (0.2.6)\n",
            "Collecting incremental>=16.10.1 (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/1d/c98a587dc06e107115cf4a58b49de20b19222c83d75335a192052af4c4b7/incremental-17.5.0-py2.py3-none-any.whl\n",
            "Collecting hyperlink>=17.1.1 (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/7f/91/e916ca10a2de1cb7101a9b24da546fb90ee14629e23160086cf3361c4fb8/hyperlink-19.0.0-py2.py3-none-any.whl\n",
            "Collecting constantly>=15.1 (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/b9/65/48c1909d0c0aeae6c10213340ce682db01b48ea900a7d9fce7a7910ff318/constantly-15.1.0-py2.py3-none-any.whl\n",
            "Collecting PyHamcrest>=1.9.0 (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/d5/d37fd731b7d0e91afcc84577edeccf4638b4f9b82f5ffe2f8b62e2ddc609/PyHamcrest-1.9.0-py2.py3-none-any.whl (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 27.6MB/s \n",
            "\u001b[?25hCollecting zope.interface>=4.4.2 (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/17/1d198a6aaa9aa4590862fe3d3a2ed7dd808050cab4eebe8a2f2f813c1376/zope.interface-4.6.0-cp36-cp36m-manylinux1_x86_64.whl (167kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 45.6MB/s \n",
            "\u001b[?25hCollecting Automat>=0.3.0 (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/a3/86/14c16bb98a5a3542ed8fed5d74fb064a902de3bdd98d6584b34553353c45/Automat-0.7.0-py2.py3-none-any.whl\n",
            "Collecting asn1crypto>=0.21.0 (from cryptography->service-identity->scrapy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/cd/35485615f45f30a510576f1a56d1e0a7ad7bd8ab5ed7cdc600ef7cd06222/asn1crypto-0.24.0-py2.py3-none-any.whl (101kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 13.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography->service-identity->scrapy) (1.12.3)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.6/dist-packages (from hyperlink>=17.1.1->Twisted>=13.1.0; python_version != \"3.4\"->scrapy) (2.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from PyHamcrest>=1.9.0->Twisted>=13.1.0; python_version != \"3.4\"->scrapy) (41.2.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography->service-identity->scrapy) (2.19)\n",
            "Building wheels for collected packages: PyDispatcher\n",
            "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-cp36-none-any.whl size=11516 sha256=e5be609242ab0d4d51d5154fb02a533f01ede2c1b22ab0ae9dc2b60109591d1d\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/99/96/cfef6665f9cb1522ee6757ae5955feedf2fe25f1737f91fa7f\n",
            "Successfully built PyDispatcher\n",
            "Installing collected packages: w3lib, cssselect, parsel, PyDispatcher, asn1crypto, cryptography, service-identity, pyOpenSSL, incremental, hyperlink, constantly, PyHamcrest, zope.interface, Automat, Twisted, queuelib, scrapy\n",
            "Successfully installed Automat-0.7.0 PyDispatcher-2.0.5 PyHamcrest-1.9.0 Twisted-19.7.0 asn1crypto-0.24.0 constantly-15.1.0 cryptography-2.7 cssselect-1.1.0 hyperlink-19.0.0 incremental-17.5.0 parsel-1.5.2 pyOpenSSL-19.0.0 queuelib-1.5.0 scrapy-1.7.3 service-identity-18.1.0 w3lib-1.21.0 zope.interface-4.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDyOVyELJxyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scrapy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVGwuT20J81w",
        "colab_type": "code",
        "outputId": "83a02c54-0dac-4014-dd23-49b7962d22c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!scrapy startproject olx"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New Scrapy project 'olx', using template directory '/usr/local/lib/python3.6/dist-packages/scrapy/templates/project', created in:\n",
            "    /content/olx\n",
            "\n",
            "You can start your first spider with:\n",
            "    cd olx\n",
            "    scrapy genspider example example.com\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOs5urFSdtWw",
        "colab_type": "code",
        "outputId": "1db4f72e-9e18-4e00-f48a-815bd724b144",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!cd olx\n",
        "!scrapy genspider example example.com"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created spider 'example' using template 'basic' \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIM8i4Hsd7B3",
        "colab_type": "code",
        "outputId": "545a739e-362f-4136-c25a-383df069bd9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "!ls -l sample_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 55504\n",
            "-r-xr-xr-x 1 root root     1697 Jan  1  2000 anscombe.json\n",
            "-rw-r--r-- 1 root root   301141 Jan  3 17:15 california_housing_test.csv\n",
            "-rw-r--r-- 1 root root  1706430 Jan  3 17:15 california_housing_train.csv\n",
            "-rw-r--r-- 1 root root 18289443 Jan  3 17:15 mnist_test.csv\n",
            "-rw-r--r-- 1 root root 36523880 Jan  3 17:15 mnist_train_small.csv\n",
            "-r-xr-xr-x 1 root root      903 Jan  1  2000 README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYSuGmegeJ0C",
        "colab_type": "code",
        "outputId": "baf2ec77-d5a7-4b62-83ed-52ffe895880b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nW1rddw9Jxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "class MarilynMansonQuotes(scrapy.Spider):\n",
        "    name = \"MarilynMansonQuotes\"\n",
        "    start_urls = [\n",
        "        'https://en.wikiquote.org/wiki/Marilyn_Manson',\n",
        "    ]\n",
        "    \n",
        "    def parse(self, response):\n",
        "        print(type(response))\n",
        "        print(dir(response))\n",
        "        for quote in response.css('div.mw-parser-output > ul > li'):\n",
        "            yield {'quote': quote.extract()}\n",
        "\n",
        "process = CrawlerProcess()\n",
        "process.crawl(MarilynMansonQuotes)\n",
        "process.start()\n",
        "process.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd6P2pV53C7y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "process.stop()\n",
        "process=None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVAORVTA3t-B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ef321af4-4fcd-49bd-8166-b35b0aeffbb8"
      },
      "source": [
        "help(scrapy.Spider.parse)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on function parse in module scrapy.spiders:\n",
            "\n",
            "parse(self, response)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZOaqMgW386m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scrapy\n",
        "import scrapy.crawler as crawler\n",
        "from multiprocessing import Process, Queue\n",
        "from twisted.internet import reactor\n",
        "\n",
        "# your spider\n",
        "class QuotesSpider(scrapy.Spider):\n",
        "    name = \"quotes\"\n",
        "    start_urls = ['http://quotes.toscrape.com/tag/humor/']\n",
        "\n",
        "    def parse(self, response):\n",
        "        for quote in response.css('div.quote'):\n",
        "            print(quote.css('span.text::text').extract_first())\n",
        "\n",
        "\n",
        "# the wrapper to make it run more times\n",
        "def run_spider(spider):\n",
        "    def f(q):\n",
        "        try:\n",
        "            runner = crawler.CrawlerRunner()\n",
        "            deferred = runner.crawl(spider)\n",
        "            deferred.addBoth(lambda _: reactor.stop())\n",
        "            reactor.run()\n",
        "            q.put(None)\n",
        "        except Exception as e:\n",
        "            q.put(e)\n",
        "\n",
        "    q = Queue()\n",
        "    p = Process(target=f, args=(q,))\n",
        "    p.start()\n",
        "    result = q.get()\n",
        "    p.join()\n",
        "\n",
        "    if result is not None:\n",
        "        raise result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iynIGG19Lbp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "d4e289fe-5252-49db-8f5b-d28b12eb3e14"
      },
      "source": [
        "run_spider(QuotesSpider)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”\n",
            "“A day without sunshine is like, you know, night.”\n",
            "“Anyone who thinks sitting in church can make you a Christian must also think that sitting in a garage can make you a car.”\n",
            "“Beauty is in the eye of the beholder and it may be necessary from time to time to give a stupid or misinformed beholder a black eye.”\n",
            "“All you need is love. But a little chocolate now and then doesn't hurt.”\n",
            "“Remember, we're madly in love, so it's all right to kiss me anytime you feel like it.”\n",
            "“Some people never go crazy. What truly horrible lives they must lead.”\n",
            "“The trouble with having an open mind, of course, is that people will insist on coming along and trying to put things in it.”\n",
            "“Think left and think right and think low and think high. Oh, the thinks you can think up if only you try!”\n",
            "“The reason I talk to myself is because I’m the only one whose answers I accept.”\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cGB0THR9cIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scrapy\n",
        "import scrapy.crawler as crawler\n",
        "from multiprocessing import Process, Queue\n",
        "from twisted.internet import reactor\n",
        "\n",
        "# your spider\n",
        "class ZacksSpider(scrapy.Spider):\n",
        "    name = \"Zacks\"\n",
        "    start_urls = ['http://www.zacks.com/stocks/']\n",
        "\n",
        "    def parse(self, response):\n",
        "        print(\"start parsing\")\n",
        "        print(type(response))\n",
        "        print(response.xpath(\"//*[contains(@id, 'main_content')]\"))\n",
        "        for quote in response.css('div.hide'):\n",
        "            print(\"in main_content\")\n",
        "            print(quote.css('span.text::text').extract_first())\n",
        "\n",
        "\n",
        "# the wrapper to make it run more times\n",
        "def run_spider(spider):\n",
        "    def f(q):\n",
        "        try:\n",
        "            runner = crawler.CrawlerRunner()\n",
        "            deferred = runner.crawl(spider)\n",
        "            deferred.addBoth(lambda _: reactor.stop())\n",
        "            reactor.run()\n",
        "            q.put(None)\n",
        "        except Exception as e:\n",
        "            q.put(e)\n",
        "\n",
        "    q = Queue()\n",
        "    p = Process(target=f, args=(q,))\n",
        "    p.start()\n",
        "    result = q.get()\n",
        "    p.join()\n",
        "\n",
        "    if result is not None:\n",
        "        raise result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pesLfbGV_UnQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "95e49a77-92ed-461e-ccd0-c9fa73a91314"
      },
      "source": [
        "run_spider(ZacksSpider)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start parsing\n",
            "<class 'scrapy.http.response.html.HtmlResponse'>\n",
            "[<Selector xpath=\"//*[contains(@id, 'main_content')]\" data='<div id=\"main_content\" role=\"main\">\\n\\n...'>]\n",
            "in main_content\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ev_kLADq_X6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}